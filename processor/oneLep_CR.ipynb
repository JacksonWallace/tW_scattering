{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from klepto.archives import dir_archive\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import coffea.processor as processor\n",
    "from coffea.processor.accumulator import AccumulatorABC\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "from coffea.btag_tools import BTagScaleFactor\n",
    "from coffea import hist\n",
    "import pandas as pd\n",
    "import uproot_methods\n",
    "import uproot\n",
    "import awkward\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from Tools.config_helpers import *\n",
    "from Tools.helpers import mergeArray, mt\n",
    "\n",
    "from Tools.objects import Collections\n",
    "from Tools.cutflow import Cutflow\n",
    "\n",
    "# This just tells matplotlib not to open any\n",
    "# interactive windows.\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting aesthetics\n",
    "\n",
    "lineopts = {\n",
    "    'color': 'r',\n",
    "}\n",
    "\n",
    "data_err_opts = {\n",
    "    'linestyle': 'none',\n",
    "    'marker': '_',\n",
    "    'markersize': 10.,\n",
    "    'color': 'r',\n",
    "    'elinewidth': 1,\n",
    "} \n",
    "\n",
    "lineopts2 = {\n",
    "    'color': [('#7FB069'), ('#5171A5') ,('#E2AEDD'), ('#A33B20'), ('#680E4B'), ('#F6AE2D'),('#45503B')],\n",
    "}\n",
    "fillopts1 = {\n",
    "    'edgecolor': (0,0,0,0.3),\n",
    "    'facecolor': [('#7FB069'), ('#5171A5') ,('#E2AEDD'), ('#A33B20'), ('#680E4B'), ('#F6AE2D'),('#45503B')],\n",
    "    #'facecolor': [('#1467cc'), ('#51d673') ,('#f7d969'), ('#af84f0'), ('#4f842e'), ('#1ff4ff'),('#3612ab')],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_flatten(val): \n",
    "    try:\n",
    "        return val.pad(1, clip=True).fillna(0.).flatten()#.reshape(-1, 1)\n",
    "    except AttributeError:\n",
    "        return val.flatten()\n",
    "\n",
    "def savefig(hists, outdir, name):\n",
    "    import re\n",
    "    bkgonly = re.compile('(?!mC750_l1)')\n",
    "    ax = hist.plot1d(hists[bkgonly], overlay=\"dataset\", density=False, stack=True)#, \n",
    "                #fill_opts = fillopts1, overflow = 'over')\n",
    "    hist.plot1d(hists['mC750_l1'], overlay=\"dataset\", density=False, stack=False, \n",
    "                error_opts=data_err_opts)#, overflow = 'over') \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim(0.001,1000000)\n",
    "    ax.figure.savefig(os.path.join(outdir, \"{}_log.pdf\".format(name)))\n",
    "    ax.clear()\n",
    "\n",
    "def savefigshape(hists, outdir, name):\n",
    "    ax = hist.plot1d(hists, overlay=\"dataset\", density=True, stack=False, \n",
    "                line_opts = lineopts2, overflow = 'over')\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim(0.00001,10)\n",
    "    ax.figure.savefig(os.path.join(outdir, \"{}_shape_log.pdf\".format(name)))\n",
    "    ax.clear()\n",
    "\n",
    "#os.environ['KERAS_BACKEND'] = 'theano'\n",
    "#from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "print(sys.getrecursionlimit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class analysisProcessor(processor.ProcessorABC):\n",
    "    \"\"\"Processor used for running the analysis\"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        ## load b-tag SFs\n",
    "        #self.btag_sf = BTagScaleFactor(os.path.expandvars(\"$TWHOME/data/DeepCSV_102XSF_V1.btag.csv.gz\", \"reshape\")\n",
    "\n",
    "        ## load the NN\n",
    "        #self.model = load_model('../ML/data/training.h5')\n",
    "        #self.stds  = pd.read_json('../ML/data/stds.json').squeeze()\n",
    "        #self.means = pd.read_json('../ML/data/means.json').squeeze()\n",
    "        \n",
    "        # we can use a large number of bins and rebin later\n",
    "        dataset_axis        = hist.Cat(\"dataset\",   \"Primary dataset\")\n",
    "        pt_axis             = hist.Bin(\"pt\",        r\"$p_{T}$ (GeV)\", 1000, 0, 1000)\n",
    "        p_axis              = hist.Bin(\"p\",         r\"$p$ (GeV)\", 1000, 0, 2500)\n",
    "        ht_axis             = hist.Bin(\"ht\",        r\"$H_{T}$ (GeV)\", 500, 0, 5000)\n",
    "        mass_axis           = hist.Bin(\"mass\",      r\"M (GeV)\", 1000, 0, 2000)\n",
    "        eta_axis            = hist.Bin(\"eta\",       r\"$\\eta$\", 60, -5.5, 5.5)\n",
    "        delta_axis          = hist.Bin(\"delta\",     r\"$\\delta$\", 100,0,10 )\n",
    "        multiplicity_axis   = hist.Bin(\"multiplicity\",         r\"N\", 20, -0.5, 19.5)\n",
    "        norm_axis           = hist.Bin(\"norm\",         r\"N\", 25, 0, 1)\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            \"MET_pt_baseline\" :          hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"HT_baseline\" :              hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            \"mtb_min_baseline\" :         hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"MET_pt\" :          hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"HT\" :              hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            \"mtb_min\" :         hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"MET_pt_CR\" :       hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"HT_CR\" :           hist.Hist(\"Counts\", dataset_axis, ht_axis),\n",
    "            \"mtb_min_CR\" :      hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            \"lead_AK8_pt\" :     hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"W_pt\" :            hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"H_pt\" :            hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            \"W_eta\" :           hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \"H_eta\" :           hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \n",
    "            \"N_b\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_AK4\" :           hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_AK8\" :           hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_H\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \"N_W\" :             hist.Hist(\"Counts\", dataset_axis, multiplicity_axis),\n",
    "            \n",
    "            \"WH_deltaPhi\":      hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            \"WH_deltaR\":        hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            \"bb_deltaPhi\":      hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            \"bb_deltaR\":        hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            \"min_dphiJetMet4\":  hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            \"dphiDiJet\":        hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            \"dphiDiFatJet\":     hist.Hist(\"Counts\", dataset_axis, delta_axis),\n",
    "            \n",
    "            'mC750_l1':         processor.defaultdict_accumulator(int),\n",
    "            'WJets':            processor.defaultdict_accumulator(int),\n",
    "            'QCD':              processor.defaultdict_accumulator(int),\n",
    "            'TTJets':           processor.defaultdict_accumulator(int),\n",
    "            'ZNuNu':            processor.defaultdict_accumulator(int),\n",
    "            'ST':               processor.defaultdict_accumulator(int),\n",
    "            'ttW/ttZ':          processor.defaultdict_accumulator(int),\n",
    "            'WW/WZ/ZZ':         processor.defaultdict_accumulator(int),\n",
    "            'LL':               processor.defaultdict_accumulator(int),\n",
    "            'totalEvents':      processor.defaultdict_accumulator(int),\n",
    "            'test1':            processor.defaultdict_accumulator(float),\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, df):\n",
    "        \"\"\"\n",
    "        Processing function. This is where the actual analysis happens.\n",
    "        \"\"\"\n",
    "        output = self.accumulator.identity()\n",
    "        dataset = df[\"dataset\"]\n",
    "        cfg = loadConfig()\n",
    "        \n",
    "        ## MET -> can switch to puppi MET\n",
    "        met_pt  = df[\"MET_pt\"]\n",
    "        met_phi = df[\"MET_phi\"]\n",
    "        \n",
    "        ## Muons\n",
    "        muon = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nMuon'],\n",
    "            pt = df['Muon_pt'].content,\n",
    "            eta = df['Muon_eta'].content,\n",
    "            phi = df['Muon_phi'].content,\n",
    "            mass = df['Muon_mass'].content,\n",
    "            miniPFRelIso_all=df['Muon_miniPFRelIso_all'].content,\n",
    "            looseId =df['Muon_looseId'].content\n",
    "            )\n",
    "        muon = muon[(muon.pt > 10) & (abs(muon.eta) < 2.4) & (muon.looseId) & (muon.miniPFRelIso_all < 0.2)]\n",
    "        #muon = Collections(df, \"Muon\", \"tightTTH\").get() # this needs a fix for DASK\n",
    "        \n",
    "        electrons = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nElectron'],\n",
    "            pt=df['Electron_pt'].content, \n",
    "            eta=df['Electron_eta'].content, \n",
    "            phi=df['Electron_phi'].content,\n",
    "            mass=df['Electron_mass'].content,\n",
    "            pdgid=df['Electron_pdgId'].content,\n",
    "            mini_iso=df['Electron_miniPFRelIso_all'].content\n",
    "        )\n",
    "        \n",
    "        ## Electrons\n",
    "        electron = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nElectron'],\n",
    "            pt = df['Electron_pt'].content,\n",
    "            eta = df['Electron_eta'].content,\n",
    "            phi = df['Electron_phi'].content,\n",
    "            mass = df['Electron_mass'].content,\n",
    "            miniPFRelIso_all=df['Electron_miniPFRelIso_all'].content,\n",
    "            cutBased=df['Electron_cutBased'].content\n",
    "            )\n",
    "        electron = electron[(electron.pt>10) & (abs(electron.eta) < 2.4) & (electron.miniPFRelIso_all < 0.1) &  (electron.cutBased >= 1)]\n",
    "        #electron = Collections(df, \"Electron\", \"tightTTH\").get() # this needs a fix for DASK\n",
    "        \n",
    "        ## FatJets\n",
    "        fatjet = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nFatJet'],\n",
    "            pt = df['FatJet_pt'].content,\n",
    "            eta = df['FatJet_eta'].content,\n",
    "            phi = df['FatJet_phi'].content,\n",
    "            mass = df['FatJet_mass'].content,\n",
    "            msoftdrop = df[\"FatJet_msoftdrop\"].content,  \n",
    "            deepTagMD_HbbvsQCD = df['FatJet_deepTagMD_HbbvsQCD'].content, \n",
    "            deepTagMD_WvsQCD = df['FatJet_deepTagMD_WvsQCD'].content, \n",
    "            deepTag_WvsQCD = df['FatJet_deepTag_WvsQCD'].content\n",
    "            \n",
    "        )\n",
    "        \n",
    "        leadingFatJets = fatjet[:,:2]\n",
    "        difatjet = leadingFatJets.choose(2)\n",
    "        dphiDiFatJet = np.arccos(np.cos(difatjet.i0.phi-difatjet.i1.phi))\n",
    "        \n",
    "        htag = fatjet[((fatjet.pt > 200) & (fatjet.deepTagMD_HbbvsQCD > 0.8365))]\n",
    "        htag_hard = fatjet[((fatjet.pt > 300) & (fatjet.deepTagMD_HbbvsQCD > 0.8365))]\n",
    "        \n",
    "        lead_htag = htag[htag.pt.argmax()]\n",
    "        h_mass = lead_htag.msoftdrop\n",
    "        print(h_mass)\n",
    "        \n",
    "        wtag = fatjet[((fatjet.pt > 200) & (fatjet.deepTagMD_HbbvsQCD < 0.8365) & (fatjet.deepTag_WvsQCD > 0.918))]\n",
    "        wtag_hard = fatjet[((fatjet.pt > 300) & (fatjet.deepTagMD_HbbvsQCD < 0.8365) & (fatjet.deepTag_WvsQCD > 0.918))]\n",
    "        \n",
    "        lead_wtag = wtag[wtag.pt.argmax()]\n",
    "        \n",
    "        wh = lead_htag.cross(lead_wtag)\n",
    "        wh_deltaPhi = np.arccos(wh.i0.phi - wh.i1.phi)\n",
    "        wh_deltaR = wh.i0.p4.delta_r(wh.i1.p4)\n",
    "        \n",
    "        ## Jets\n",
    "        jet = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nJet'],\n",
    "            pt = df['Jet_pt'].content,\n",
    "            eta = df['Jet_eta'].content,\n",
    "            phi = df['Jet_phi'].content,\n",
    "            mass = df['Jet_mass'].content,\n",
    "            jetId = df['Jet_jetId'].content, # https://twiki.cern.ch/twiki/bin/view/CMS/JetID\n",
    "            #puId = df['Jet_puId'].content, # https://twiki.cern.ch/twiki/bin/viewauth/CMS/PileupJetID\n",
    "            btagDeepB = df['Jet_btagDeepB'].content, # https://twiki.cern.ch/twiki/bin/viewauth/CMS/BtagRecommendation102X\n",
    "            #deepJet = df['Jet_'].content # not there yet?\n",
    "        )\n",
    "        \n",
    "        jet       = jet[(jet.pt>30) & (jet.jetId>1) & (abs(jet.eta)<2.4)]\n",
    "        jet       = jet[~jet.match(muon, deltaRCut=0.4)] # remove jets that overlap with muons\n",
    "        jet       = jet[~jet.match(electron, deltaRCut=0.4)] # remove jets that overlap with electrons\n",
    "        jet       = jet[jet.pt.argsort(ascending=False)] # sort the jets\n",
    "        btag      = jet[(jet.btagDeepB>0.4184)]\n",
    "        light     = jet[(jet.btagDeepB<0.4184)]\n",
    "        \n",
    "        ## Get the leading b-jets\n",
    "        high_score_btag = jet[jet.btagDeepB.argsort(ascending=False)][:,:2]\n",
    "        \n",
    "        leading_jet    = jet[jet.pt.argmax()]\n",
    "        leading_b      = btag[btag.pt.argmax()]\n",
    "        \n",
    "        bb = high_score_btag.choose(2)\n",
    "        bb_deltaPhi = np.arccos(np.cos(bb.i0.phi-bb.i1.phi))\n",
    "        bb_deltaR = bb.i0.p4.delta_r(bb.i1.p4)\n",
    "        \n",
    "        mtb = mt(btag.pt, btag.phi, met_pt, met_phi)\n",
    "        min_mtb = mtb.min()\n",
    "        \n",
    "        ## other variables\n",
    "        ht = jet.pt.sum()\n",
    "        \n",
    "        min_dphiJetMet4 = np.arccos(np.cos(jet[:,:4].phi-met_phi)).min()\n",
    "        \n",
    "        leadingJets = jet[:,:2]\n",
    "        dijet = leadingJets.choose(2)\n",
    "        dphiDiJet = np.arccos(np.cos(dijet.i0.phi-dijet.i1.phi))\n",
    "        \n",
    "        ## define selections (maybe move to a different file at some point)\n",
    "        \n",
    "        output['totalEvents']['all'] += len(df['weight'])\n",
    "        \n",
    "        # Cutflow\n",
    "        #processes = ['mC750_l1', 'WJets', 'QCD', 'TTJets', 'ZNuNu', 'ST', 'ttW/ttZ', 'WW/WZ/ZZ']\n",
    "        processes = ['mC750_l1', 'LL', 'QCD', 'ZNuNu']\n",
    "        cutflow = Cutflow(output, df, cfg, processes)\n",
    "        \n",
    "        cutflow.addRow( 'Exactly 1 e or mu',   ((electron.counts + muon.counts)==1) )\n",
    "        #cutflow.addRow( 'muon veto',   (muon.counts==0) )\n",
    "        #cutflow.addRow( 'MET>250',     (met_pt>250) )\n",
    "        cutflow.addRow( 'njet2',       (jet.counts>=2) )\n",
    "        cutflow.addRow( 'jetveto',       (jet.counts<=5) )\n",
    "        cutflow.addRow( 'nbtag',       (btag.counts>=1) )\n",
    "        \n",
    "        baseline = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "        #cutflow.addRow(  'min_mt_met_b>200', (min_mtb>200)  )\n",
    "        cutflow.addRow( 'min_dphiJetMet4', (min_dphiJetMet4>0.5))\n",
    "        cutflow.addRow( 'dphiDiJet', (dphiDiJet.min()<2.5) ) # the min doesn't do anything here\n",
    "        cutflow.addRow( 'dphiDiFatJet', (dphiDiFatJet<2.5).all() ) # by using .all() I do not implicitely cut on the number of fat jets\n",
    "        \n",
    "        vetoQCD = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "        cutflow.addRow( 'HT>300',      (ht>300) )\n",
    "        cutflow.addRow( 'N_fatjet>0',      (fatjet.counts>0) )\n",
    "        cutflow.addRow( 'N_htag>0',     (htag.counts>0))\n",
    "        cutflow.addRow( 'H_mass>90',     (lead_htag.msoftdrop>90))\n",
    "        cutflow.addRow( 'H_mass<150',     (h_mass<150))\n",
    "        cutflow.addRow( 'N_fatjet>1',      (fatjet.counts>1) )\n",
    "        cutflow.addRow( 'N_wtag>0',     (wtag.counts>0))\n",
    "        \n",
    "        event_selection = copy.deepcopy(cutflow.selection)\n",
    "        \n",
    "        #cutflow.addRow( 'N_htag>0 hard',     (htag_hard.counts>0))\n",
    "        cutflow.addRow( 'MET>250',     (met_pt>250) )\n",
    "        cutflow.addRow( 'MET>400',     (met_pt>400) )\n",
    "        cutflow.addRow( 'MET>600',     (met_pt>600) )\n",
    "\n",
    "        # signal enriched selection of events\n",
    "        signal_selection = cutflow.selection\n",
    "        \n",
    "        ### And fill the histograms\n",
    "        output['MET_pt_baseline'].fill(dataset=dataset, pt=met_pt[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "        output['HT_baseline'].fill(dataset=dataset, ht=ht[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "        output['mtb_min_baseline'].fill(dataset=dataset, mass=mtb[baseline].min().flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "\n",
    "        output['MET_pt'].fill(dataset=dataset, pt=met_pt[vetoQCD].flatten(), weight=df['weight'][vetoQCD]*cfg['lumi'])\n",
    "        output['HT'].fill(dataset=dataset, ht=ht[vetoQCD].flatten(), weight=df['weight'][vetoQCD]*cfg['lumi'])\n",
    "        output['mtb_min'].fill(dataset=dataset, mass=mtb[vetoQCD].min().flatten(), weight=df['weight'][vetoQCD]*cfg['lumi'])\n",
    "        \n",
    "        ## N jet and N b without selections on those\n",
    "        output['N_AK4'].fill(dataset=dataset, multiplicity=jet[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])\n",
    "        output['N_b'].fill(dataset=dataset, multiplicity=btag[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "        output['N_W'].fill(dataset=dataset, multiplicity=htag[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "        output['N_H'].fill(dataset=dataset, multiplicity=wtag[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "        output['N_AK8'].fill(dataset=dataset, multiplicity=fatjet[baseline].counts, weight=df['weight'][baseline]*cfg['lumi'])       \n",
    "\n",
    "        output['bb_deltaPhi'].fill(dataset=dataset, delta=bb_deltaPhi[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "        output['bb_deltaR'].fill(dataset=dataset, delta=bb_deltaR[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "\n",
    "        output['min_dphiJetMet4'].fill(dataset=dataset, delta=min_dphiJetMet4[baseline].flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "        output['dphiDiJet'].fill(dataset=dataset, delta=dphiDiJet[baseline].min().flatten(), weight=df['weight'][baseline]*cfg['lumi'])\n",
    "\n",
    "        ## Higgs and W pt\n",
    "        output['lead_AK8_pt'].fill(dataset=dataset, pt=fatjet[(baseline & (fatjet.counts>0))].pt.max().flatten(), weight=df['weight'][(baseline & (fatjet.counts>0))]*cfg['lumi'])\n",
    "        output['dphiDiFatJet'].fill(dataset=dataset, delta=dphiDiFatJet[(baseline & (fatjet.counts>1))].min().flatten(), weight=df['weight'][(baseline & (fatjet.counts>1))]*cfg['lumi'])\n",
    "\n",
    "        output['H_pt'].fill(dataset=dataset, pt=lead_htag[event_selection].pt.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['H_eta'].fill(dataset=dataset, eta=lead_htag[event_selection].eta.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "\n",
    "        output['W_pt'].fill(dataset=dataset, pt=lead_wtag[event_selection].pt.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['W_eta'].fill(dataset=dataset, eta=lead_wtag[event_selection].eta.flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "\n",
    "        output['WH_deltaPhi'].fill(dataset=dataset, delta=wh_deltaPhi[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['WH_deltaR'].fill(dataset=dataset, delta=wh_deltaR[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "\n",
    "        output['MET_pt_CR'].fill(dataset=dataset, pt=met_pt[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['HT_CR'].fill(dataset=dataset, ht=ht[event_selection].flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        output['mtb_min_CR'].fill(dataset=dataset, mass=mtb[event_selection].min().flatten(), weight=df['weight'][event_selection]*cfg['lumi'])\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "runLocal = True\n",
    "\n",
    "\n",
    "if not runLocal:\n",
    "    # Get the scheduler from the dask_cluster notebook\n",
    "    from dask.distributed import Client, progress\n",
    "\n",
    "    c = Client('tcp://169.228.130.5:27879')\n",
    "\n",
    "    ## for dask\n",
    "    exe_args = {\n",
    "        'client': c,\n",
    "        #'savemetrics': True,\n",
    "    }\n",
    "    exe = processor.dask_executor\n",
    "    \n",
    "else:\n",
    "    ## for local\n",
    "    exe_args = {\n",
    "        'workers': 4,\n",
    "        'function_args': {'flatten': False}\n",
    "    }\n",
    "    exe = processor.futures_executor\n",
    "\n",
    "if not runLocal:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beee7175912f43a391c5521e6061bfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(HTML(value='Processing'), FloatProgress(value=0.0, max=495.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[] [] [] ... [] [] []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/awkward/array/jagged.py:1043: RuntimeWarning: invalid value encountered in arccos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[] [] [] ... [] [] []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/awkward/array/jagged.py:1043: RuntimeWarning: invalid value encountered in arccos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[] [] [] ... [] [] []]\n",
      "[[] [] [] ... [] [] []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/awkward/array/jagged.py:1043: RuntimeWarning: invalid value encountered in arccos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/awkward/array/jagged.py:1043: RuntimeWarning: invalid value encountered in arccos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[] [] [] ... [] [] []]\n",
      "[[] [89.0625] [] ... [] [] []]\n",
      "[[] [] [] ... [] [] []]\n",
      "[[] [] [] ... [] [] [61.5]]\n",
      "[[] [] [] ... [] [] []]\n",
      "[[] [] [] ... [] [] []]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in-place operations not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/cvmfs/cms.cern.ch/slc6_amd64_gcc700/external/python3/3.6.4-gnimlf/lib/python3.6/concurrent/futures/process.py\", line 175, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/coffea/processor/executor.py\", line 139, in __call__\n    out = self.function(*args, **kwargs)\n  File \"/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/coffea/processor/executor.py\", line 833, in _work_function\n    raise e\n  File \"/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/coffea/processor/executor.py\", line 794, in _work_function\n    out = processor_instance.process(df)\n  File \"<ipython-input-14-a0e0d7f9e28e>\", line 223, in process\n    cutflow.addRow( 'H_mass>90',     (lead_htag.msoftdrop>90))\n  File \"/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/Tools/cutflow.py\", line 17, in addRow\n    self.selection &= selection\n  File \"/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/awkward/array/jagged.py\", line 949, in __array_ufunc__\n    raise NotImplementedError(\"in-place operations not supported\")\nNotImplementedError: in-place operations not supported\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2acc436b35b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m                                       \u001b[0mexecutor_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexe_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                                       \u001b[0;31m#chunksize=250000,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                                       \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                                      )\n\u001b[1;32m     75\u001b[0m     \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fileset'\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mfileset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mrun_uproot_job\u001b[0;34m(fileset, treename, processor_instance, executor, executor_args, pre_executor, pre_args, chunksize, maxchunks, metadata_cache)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     }\n\u001b[1;32m   1076\u001b[0m     \u001b[0mexe_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m     \u001b[0mexecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexe_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m     \u001b[0mwrapped_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chunks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mfutures_executor\u001b[0;34m(items, function, accumulator, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0m_futures_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtailtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/coffeaEnv/lib/python3.6/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36m_futures_handler\u001b[0;34m(futures_set, output, status, unit, desc, add_fn, tailtimeout)\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mfutures_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                     \u001b[0madd_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m                     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mlast_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc700/external/python3/3.6.4-gnimlf/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc700/external/python3/3.6.4-gnimlf/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in-place operations not supported"
     ]
    }
   ],
   "source": [
    "overwrite = True\n",
    "small = False\n",
    "\n",
    "tag = '0p1p20'\n",
    "#tag = '0p1p16/2018'\n",
    "\n",
    "fileset_WH   = {'mC750_l1': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/WH_had_750_1_nanoAOD/*.root'),\n",
    "                'WJets': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/WJetsToLNu*/*.root'),\n",
    "                'QCD': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/QCD_HT*/*.root'),\n",
    "                'TTJets': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/TTJets*/*.root'),\n",
    "                'ZNuNu': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ZJetsToNuNu*/*.root'),\n",
    "                'ST': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ST*/*.root'),\n",
    "                'ttW/ttZ': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ttWJets*/*.root')\n",
    "                    +glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ttZJets*/*.root'),\n",
    "                'WW/WZ/ZZ': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/WW*/*.root')\n",
    "                    +glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/WZ*/*.root')\n",
    "                    +glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ZZTo2L2Nu*/*.root')\n",
    "                    +glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ZZTo2Q2Nu*/*.root')\n",
    "                }\n",
    "\n",
    "fileset_WH_merge = {'mC750_l1': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/WH_had_750_1_nanoAOD/*.root'),\n",
    "                'LL': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/WJetsToLNu*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/TTJets*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ST*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ttWJets*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/WW*/*.root'),\n",
    "                'QCD': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/QCD_HT*/*.root'),\n",
    "                'ZNuNu': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ZJetsToNuNu*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ttZJets*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/WZ*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ZZTo2L2Nu*/*.root')\n",
    "                    + glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ZZTo2Q2Nu*/*.root')\n",
    "\n",
    "                }\n",
    "\n",
    "\n",
    "# load the config and the cache\n",
    "cfg = loadConfig()\n",
    "\n",
    "cacheName = 'WH_small' if small else 'WH'\n",
    "\n",
    "# histograms\n",
    "histograms = []\n",
    "histograms += ['N_AK4']\n",
    "\n",
    "# initialize cache\n",
    "cache = dir_archive(os.path.join(os.path.expandvars(cfg['caches']['base']), cfg['caches'][cacheName]), serialized=True)\n",
    "if not overwrite:\n",
    "    cache.load()\n",
    "\n",
    "if cfg == cache.get('cfg') and histograms == cache.get('histograms') and cache.get('simple_output'):\n",
    "    output = cache.get('simple_output')\n",
    "\n",
    "else:\n",
    "    # Run the processor\n",
    "    if small:\n",
    "        fileset = {\n",
    "                    'mC750_l1': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/WH_had_750_1_nanoAOD/*.root'),\n",
    "                    'LL': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/ttWJets*/*.root')[:2],\n",
    "                    'QCD': glob.glob('/hadoop/cms/store/user/ksalyer/allHadTest/'+tag+'/QCD_HT*/*.root')[:2]}\n",
    "        workers = 4\n",
    "    else:\n",
    "        fileset = fileset_WH_merge\n",
    "        workers = 16\n",
    "    \n",
    "        \n",
    "    output = processor.run_uproot_job(fileset,\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=analysisProcessor(),\n",
    "                                      executor=exe,\n",
    "                                      executor_args=exe_args,\n",
    "                                      #chunksize=250000,\n",
    "                                      chunksize=100000,\n",
    "                                     )\n",
    "    cache['fileset']        = fileset\n",
    "    cache['cfg']            = cfg\n",
    "    cache['histograms']     = histograms\n",
    "    cache['simple_output']  = output\n",
    "    cache.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutflow\n",
    "from Tools.helpers import getCutFlowTable\n",
    "\n",
    "#processes = ['mC750_l1', 'WJets', 'QCD', 'TTJets', 'ZNuNu', 'ST', 'ttW/ttZ', 'WW/WZ/ZZ']\n",
    "processes = ['mC750_l1', 'LL', 'QCD', 'ZNuNu']\n",
    "lines     = ['entry']\n",
    "lines    += ['Exactly 1 e or mu', 'njet2', 'jetveto', 'nbtag',  'HT>300', 'min_dphiJetMet4', 'dphiDiJet', 'dphiDiFatJet', 'N_fatjet>0', 'N_htag>0', 'H_mass>90', 'H_mass<150',  'N_fatjet>1', 'N_wtag>0',  'MET>250', 'MET>400', 'MET>600']\n",
    "df        = getCutFlowTable(output, processes=processes, lines=lines, significantFigures=4, signal='mC750_l1')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiencies\n",
    "df = getCutFlowTable(output, processes=processes, lines=lines, significantFigures=3, absolute=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show/save histograms\n",
    "histogram = output['MET_pt_CR']\n",
    "ax = hist.plot1d(histogram,overlay=\"dataset\", stack=True)\n",
    "new_met_bins = hist.Bin('pt', r'MET', 50, 200, 1000)\n",
    "histogram = histogram.rebin('pt', new_met_bins)\n",
    "\n",
    "#plt.savefig('plots/singleLep_CR/MET_pt_CR.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desired histograms to save\n",
    "histograms = [\"MET_pt_CR\",\n",
    "              \"H_pt\",\n",
    "              \"H_eta\",\n",
    "              \"W_pt\",\n",
    "              \"W_eta\",\n",
    "              #\"ht\", \n",
    "              #\"jet_pt\", \n",
    "              #\"njets\", \n",
    "              #\"bjets\",\n",
    "             ]\n",
    "\n",
    "outdir = \"/home/users/ksalyer/CMSSW_10_2_9/src/tW_scattering/plots/singleLeptonCR/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in histograms:\n",
    "    print (name)\n",
    "    hists = output[name]\n",
    "    \n",
    "    if name == \"MET_pt_CR\":\n",
    "        new_met_bins = hist.Bin('pt', r'$p_{T}^{missing}$', 50, 200, 1000)\n",
    "        hists = hists.rebin('pt', new_met_bins)\n",
    "    \n",
    "    if name == \"H_pt\":\n",
    "        new_pt_bins = hist.Bin('pt', r'Higgs $p_{T}$', 50, 200, 1000)\n",
    "        hists = hists.rebin('pt', new_pt_bins)\n",
    "    \n",
    "    if name == \"H_eta\":\n",
    "        new_eta_bins = hist.Bin('eta', r'Higgs $\\eta$', 20, -4, 4)\n",
    "        hists = hists.rebin('eta', new_eta_bins)\n",
    "    \n",
    "    if name == \"W_pt\":\n",
    "        new_pt_bins = hist.Bin('pt', r'W $p_{T}$', 50, 200, 1000)\n",
    "        hists = hists.rebin('pt', new_pt_bins)\n",
    "    \n",
    "    if name == \"W_eta\":\n",
    "        new_eta_bins = hist.Bin('eta', r'W $\\eta$', 20, -4, 4)\n",
    "        hists = hists.rebin('eta', new_eta_bins)\n",
    "        \n",
    "    '''if name == \"ht\":\n",
    "        new_ht_bins = hist.Bin('pt', r'HT', 25, 300, 2000)\n",
    "        hists = hists.rebin('pt', new_ht_bins)\n",
    "        \n",
    "    if name == \"jet_pt\":\n",
    "        new_jetpt_bins = hist.Bin('pt', r'jet $p_{T}$', 25, 300, 2000)\n",
    "        hists = hists.rebin('pt', new_jetpt_bins)\n",
    "        \n",
    "    if name == \"njets\":\n",
    "        new_nj_bins = hist.Bin('multiplicity', r'nJets', 16, -0.5, 15.5)\n",
    "        hists = hists.rebin('multiplicity', new_nj_bins)\n",
    "        \n",
    "    if name == \"bjets\":\n",
    "        new_bj_bins = hist.Bin('multiplicity', r'nBJets', 7, -0.5, 6.5)\n",
    "        hists = hists.rebin('multiplicity', new_bj_bins)\n",
    "    '''\n",
    "        \n",
    "    savefig(hists,outdir, name)\n",
    "    #savefigshape(hists,outdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffeaEnv",
   "language": "python",
   "name": "coffeaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
